<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Large Language Models: Architecture & Training</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            overflow: hidden;
        }

        .presentation {
            width: 100vw;
            height: 100vh;
            position: relative;
        }

        .slide {
            width: 100%;
            height: 100%;
            display: none;
            padding: 60px 80px;
            background: white;
            position: absolute;
            top: 0;
            left: 0;
        }

        .slide.active {
            display: flex;
            flex-direction: column;
            justify-content: center;
            animation: slideIn 0.5s ease-out;
        }

        @keyframes slideIn {
            from {
                opacity: 0;
                transform: translateX(50px);
            }
            to {
                opacity: 1;
                transform: translateX(0);
            }
        }

        .slide h1 {
            font-size: 3.5em;
            color: #2c3e50;
            margin-bottom: 30px;
            border-bottom: 4px solid #667eea;
            padding-bottom: 20px;
        }

        .slide h2 {
            font-size: 2.5em;
            color: #34495e;
            margin-bottom: 40px;
        }

        .slide h3 {
            font-size: 1.8em;
            color: #667eea;
            margin: 30px 0 20px 0;
        }

        .slide p, .slide li {
            font-size: 1.3em;
            line-height: 1.8;
            color: #2c3e50;
            margin-bottom: 15px;
        }

        .slide ul, .slide ol {
            margin-left: 40px;
            margin-bottom: 20px;
        }

        .slide li {
            margin-bottom: 15px;
        }

        .slide code {
            background: #f4f4f4;
            padding: 2px 8px;
            border-radius: 4px;
            font-family: 'Courier New', monospace;
            color: #e74c3c;
        }

        .slide pre {
            background: #2c3e50;
            color: #ecf0f1;
            padding: 20px;
            border-radius: 8px;
            overflow-x: auto;
            margin: 20px 0;
            font-size: 1.1em;
        }

        .title-slide {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            text-align: center;
            justify-content: center;
            align-items: center;
        }

        .title-slide h1 {
            color: white;
            border: none;
            font-size: 4em;
        }

        .title-slide p {
            color: rgba(255, 255, 255, 0.9);
            font-size: 1.5em;
        }

        .controls {
            position: fixed;
            bottom: 30px;
            right: 30px;
            z-index: 1000;
        }

        .controls button {
            background: #667eea;
            color: white;
            border: none;
            padding: 15px 30px;
            margin: 0 5px;
            border-radius: 5px;
            font-size: 1.1em;
            cursor: pointer;
            transition: background 0.3s;
        }

        .controls button:hover {
            background: #764ba2;
        }

        .slide-number {
            position: fixed;
            bottom: 30px;
            left: 30px;
            color: #667eea;
            font-size: 1.2em;
            font-weight: bold;
        }

        .two-column {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 40px;
            margin-top: 20px;
        }

        .highlight-box {
            background: #f8f9fa;
            border-left: 5px solid #667eea;
            padding: 20px;
            margin: 20px 0;
            border-radius: 5px;
        }

        .equation {
            background: #f4f4f4;
            padding: 20px;
            margin: 20px 0;
            border-radius: 8px;
            text-align: center;
            font-family: 'Times New Roman', serif;
            font-size: 1.4em;
            color: #2c3e50;
        }

        .small-text {
            font-size: 1.1em;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }

        th, td {
            border: 1px solid #ddd;
            padding: 12px;
            text-align: left;
        }

        th {
            background: #667eea;
            color: white;
        }

        tr:nth-child(even) {
            background: #f8f9fa;
        }
    </style>
</head>
<body>
    <div class="presentation">
        <!-- Slide 1: Title -->
        <div class="slide active title-slide">
            <h1>Large Language Models</h1>
            <p style="margin-top: 30px; font-size: 2em;">Architecture, Training & Inner Workings</p>
            <p style="margin-top: 50px;">A Technical Deep Dive</p>
        </div>

        <!-- Slide 2: Overview -->
        <div class="slide">
            <h1>What Are Large Language Models?</h1>
            <div class="highlight-box">
                <p><strong>Definition:</strong> Neural networks trained on massive text corpora to predict and generate human-like text</p>
            </div>
            <h3>Key Characteristics:</h3>
            <ul>
                <li><strong>Scale:</strong> Billions to trillions of parameters</li>
                <li><strong>Architecture:</strong> Primarily based on Transformer architecture</li>
                <li><strong>Training:</strong> Self-supervised learning on vast text datasets</li>
                <li><strong>Emergent Abilities:</strong> Capabilities that arise at scale (reasoning, few-shot learning)</li>
            </ul>
            <h3>Examples:</h3>
            <ul>
                <li>GPT-4, Claude, LLaMA, PaLM, Gemini</li>
            </ul>
        </div>

        <!-- Slide 3: Foundation - Neural Networks -->
        <div class="slide">
            <h1>Foundation: Neural Networks</h1>
            <h3>Basic Building Blocks:</h3>
            <ul>
                <li><strong>Neurons:</strong> Compute weighted sum of inputs + bias, apply activation function</li>
                <li><strong>Layers:</strong> Collections of neurons processing information</li>
                <li><strong>Weights & Biases:</strong> Learnable parameters adjusted during training</li>
            </ul>
            <div class="equation">
                y = activation(W·x + b)
            </div>
            <h3>Key Concepts:</h3>
            <ul>
                <li><strong>Forward Pass:</strong> Input → Hidden Layers → Output</li>
                <li><strong>Backpropagation:</strong> Calculate gradients to update weights</li>
                <li><strong>Gradient Descent:</strong> Iteratively minimize loss function</li>
            </ul>
        </div>

        <!-- Slide 4: The Transformer Revolution -->
        <div class="slide">
            <h1>The Transformer Architecture</h1>
            <div class="highlight-box">
                <p><strong>"Attention Is All You Need"</strong> (Vaswani et al., 2017)</p>
                <p>Revolutionized NLP by replacing recurrence with attention mechanisms</p>
            </div>
            <h3>Why Transformers?</h3>
            <ul>
                <li><strong>Parallelization:</strong> Process entire sequences simultaneously (vs. RNNs)</li>
                <li><strong>Long-Range Dependencies:</strong> Capture relationships across long contexts</li>
                <li><strong>Scalability:</strong> Efficient training on massive datasets with modern hardware</li>
                <li><strong>No Vanishing Gradients:</strong> Better gradient flow than RNNs/LSTMs</li>
            </ul>
        </div>

        <!-- Slide 5: Transformer Components -->
        <div class="slide">
            <h1>Transformer Components</h1>
            <div class="two-column">
                <div>
                    <h3>Encoder (e.g., BERT):</h3>
                    <ul>
                        <li>Multi-Head Self-Attention</li>
                        <li>Feed-Forward Networks</li>
                        <li>Layer Normalization</li>
                        <li>Residual Connections</li>
                    </ul>
                </div>
                <div>
                    <h3>Decoder (e.g., GPT):</h3>
                    <ul>
                        <li>Masked Self-Attention</li>
                        <li>Feed-Forward Networks</li>
                        <li>Layer Normalization</li>
                        <li>Residual Connections</li>
                    </ul>
                </div>
            </div>
            <div class="highlight-box">
                <p><strong>Most modern LLMs are decoder-only models</strong> (GPT, LLaMA, Claude)</p>
                <p>They use causal/masked attention to predict next tokens autoregressively</p>
            </div>
        </div>

        <!-- Slide 6: Self-Attention Mechanism -->
        <div class="slide">
            <h1>Self-Attention: The Core Innovation</h1>
            <h3>Purpose:</h3>
            <p>Allow each token to "attend" to all other tokens in the sequence, learning contextual relationships</p>
            
            <h3>Mechanism:</h3>
            <ol>
                <li><strong>Create Q, K, V matrices:</strong> Query, Key, Value from input embeddings</li>
                <li><strong>Compute attention scores:</strong> How much each token should attend to others</li>
                <li><strong>Apply softmax:</strong> Normalize scores to probabilities</li>
                <li><strong>Weighted sum:</strong> Combine values based on attention weights</li>
            </ol>
            
            <div class="equation">
                Attention(Q, K, V) = softmax(QK<sup>T</sup> / √d<sub>k</sub>) V
            </div>
            
            <p class="small-text">Where d<sub>k</sub> is the dimension of key vectors (scaling factor)</p>
        </div>

        <!-- Slide 7: Multi-Head Attention -->
        <div class="slide">
            <h1>Multi-Head Attention</h1>
            <h3>Concept:</h3>
            <p>Run multiple attention mechanisms in parallel, each learning different aspects of relationships</p>
            
            <div class="highlight-box">
                <p><strong>Example:</strong> One head might focus on syntactic relationships, another on semantic meaning, another on long-range dependencies</p>
            </div>
            
            <h3>Process:</h3>
            <ul>
                <li>Split embedding dimension across <em>h</em> heads</li>
                <li>Each head computes attention independently</li>
                <li>Concatenate all head outputs</li>
                <li>Project through final linear layer</li>
            </ul>
            
            <div class="equation">
                MultiHead(Q,K,V) = Concat(head₁, ..., head<sub>h</sub>)W<sup>O</sup>
            </div>
        </div>

        <!-- Slide 8: Positional Encoding -->
        <div class="slide">
            <h1>Positional Encoding</h1>
            <h3>The Problem:</h3>
            <p>Attention mechanism is <strong>permutation invariant</strong> — it doesn't inherently understand token order</p>
            
            <h3>The Solution:</h3>
            <p>Add positional information to input embeddings</p>
            
            <h3>Approaches:</h3>
            <ul>
                <li><strong>Sinusoidal (Original):</strong> Fixed sine/cosine functions at different frequencies</li>
                <li><strong>Learned:</strong> Trainable position embeddings</li>
                <li><strong>Relative (Modern):</strong> RoPE (Rotary Position Embedding), ALiBi</li>
            </ul>
            
            <div class="highlight-box">
                <p><strong>RoPE (Rotary Position Embedding):</strong> Used in LLaMA, GPT-NeoX — encodes position by rotating embeddings in complex space</p>
            </div>
        </div>

        <!-- Slide 9: Feed-Forward Networks -->
        <div class="slide">
            <h1>Feed-Forward Networks (FFN)</h1>
            <h3>Structure:</h3>
            <p>Applied to each position independently after attention layer</p>
            
            <div class="equation">
                FFN(x) = activation(xW₁ + b₁)W₂ + b₂
            </div>
            
            <h3>Characteristics:</h3>
            <ul>
                <li><strong>Two linear transformations</strong> with activation in between</li>
                <li><strong>Expansion factor:</strong> Hidden dimension typically 4x larger than model dimension</li>
                <li><strong>Activation:</strong> ReLU, GELU, SwiGLU (modern variants)</li>
                <li><strong>Purpose:</strong> Add non-linearity, process each token's representation</li>
            </ul>
            
            <div class="highlight-box">
                <p><strong>SwiGLU:</strong> Used in LLaMA — combines Swish activation with GLU (Gated Linear Units)</p>
            </div>
        </div>

        <!-- Slide 10: Layer Normalization & Residuals -->
        <div class="slide">
            <h1>Layer Normalization & Residual Connections</h1>
            
            <div class="two-column">
                <div>
                    <h3>Layer Normalization:</h3>
                    <ul>
                        <li>Normalize across feature dimension</li>
                        <li>Stabilizes training</li>
                        <li>Reduces internal covariate shift</li>
                    </ul>
                    <div class="equation" style="font-size: 1.1em;">
                        LayerNorm(x) = γ((x-μ)/σ) + β
                    </div>
                </div>
                <div>
                    <h3>Residual Connections:</h3>
                    <ul>
                        <li>Skip connections around layers</li>
                        <li>Enable gradient flow</li>
                        <li>Allow training of very deep networks</li>
                    </ul>
                    <div class="equation" style="font-size: 1.1em;">
                        output = x + Layer(x)
                    </div>
                </div>
            </div>
            
            <h3>Placement Variants:</h3>
            <ul>
                <li><strong>Post-Norm (Original):</strong> Norm after residual — x + Norm(Layer(x))</li>
                <li><strong>Pre-Norm (Modern):</strong> Norm before layer — x + Layer(Norm(x))</li>
            </ul>
        </div>

        <!-- Slide 11: Tokenization -->
        <div class="slide">
            <h1>Tokenization</h1>
            <h3>Purpose:</h3>
            <p>Convert raw text into discrete tokens that the model can process</p>
            
            <h3>Common Approaches:</h3>
            <ul>
                <li><strong>Byte-Pair Encoding (BPE):</strong> Iteratively merge most frequent character pairs</li>
                <li><strong>WordPiece:</strong> Similar to BPE, used by BERT</li>
                <li><strong>SentencePiece:</strong> Language-agnostic, treats text as raw Unicode</li>
                <li><strong>Unigram:</strong> Probabilistic approach to subword segmentation</li>
            </ul>
            
            <div class="highlight-box">
                <p><strong>Example:</strong> "tokenization" → ["token", "ization"] or ["tok", "en", "ization"]</p>
                <p>Vocabulary size: typically 32k-100k tokens</p>
            </div>
        </div>

        <!-- Slide 12: Embeddings -->
        <div class="slide">
            <h1>Token & Position Embeddings</h1>
            
            <h3>Token Embeddings:</h3>
            <ul>
                <li><strong>Lookup table:</strong> Each token ID → dense vector (e.g., 768, 1024, 4096 dimensions)</li>
                <li><strong>Learned during training:</strong> Similar tokens get similar embeddings</li>
                <li><strong>Captures semantic meaning</strong> in continuous space</li>
            </ul>
            
            <h3>Combined Input:</h3>
            <div class="equation">
                Input = TokenEmbedding(token) + PositionalEncoding(position)
            </div>
            
            <div class="highlight-box">
                <p><strong>Embedding Dimension:</strong> Determines model width (e.g., GPT-3: 12,288, LLaMA-2-7B: 4,096)</p>
            </div>
        </div>

        <!-- Slide 13: Model Architecture Summary -->
        <div class="slide">
            <h1>Complete Architecture Flow</h1>
            <ol style="font-size: 1.2em;">
                <li><strong>Input Text</strong> → Tokenization → Token IDs</li>
                <li><strong>Embedding Layer</strong> → Token Embeddings + Positional Encodings</li>
                <li><strong>Transformer Blocks</strong> (repeated N times):
                    <ul>
                        <li>Layer Norm → Multi-Head Self-Attention → Residual</li>
                        <li>Layer Norm → Feed-Forward Network → Residual</li>
                    </ul>
                </li>
                <li><strong>Final Layer Norm</strong></li>
                <li><strong>Output Projection</strong> → Logits over vocabulary</li>
                <li><strong>Softmax</strong> → Probability distribution over next token</li>
            </ol>
            
            <div class="highlight-box">
                <p><strong>Autoregressive Generation:</strong> Sample token, append to sequence, repeat</p>
            </div>
        </div>

        <!-- Slide 14: Training Objective -->
        <div class="slide">
            <h1>Training Objective: Next Token Prediction</h1>
            
            <h3>Causal Language Modeling:</h3>
            <p>Predict the next token given all previous tokens</p>
            
            <div class="equation">
                P(w<sub>t</sub> | w<sub>1</sub>, w<sub>2</sub>, ..., w<sub>t-1</sub>)
            </div>
            
            <h3>Loss Function - Cross-Entropy:</h3>
            <div class="equation">
                L = -Σ log P(w<sub>t</sub> | w<sub>&lt;t</sub>)
            </div>
            
            <h3>Why This Works:</h3>
            <ul>
                <li><strong>Self-supervised:</strong> No manual labeling needed — text provides its own labels</li>
                <li><strong>Scalable:</strong> Can train on virtually unlimited text data</li>
                <li><strong>Emergent understanding:</strong> To predict well, model must learn grammar, facts, reasoning</li>
            </ul>
        </div>

        <!-- Slide 15: Training Data -->
        <div class="slide">
            <h1>Training Data</h1>
            
            <h3>Scale:</h3>
            <ul>
                <li><strong>GPT-3:</strong> ~300B tokens (570GB of text)</li>
                <li><strong>LLaMA-2:</strong> 2 trillion tokens</li>
                <li><strong>Modern models:</strong> Multiple trillions of tokens</li>
            </ul>
            
            <h3>Sources:</h3>
            <ul>
                <li><strong>Web crawls:</strong> Common Crawl, filtered web pages</li>
                <li><strong>Books:</strong> Books1, Books2, Project Gutenberg</li>
                <li><strong>Wikipedia:</strong> High-quality encyclopedic content</li>
                <li><strong>Code:</strong> GitHub, Stack Overflow (for code-capable models)</li>
                <li><strong>Academic papers:</strong> ArXiv, PubMed</li>
            </ul>
            
            <div class="highlight-box">
                <p><strong>Data Quality > Quantity:</strong> Filtering, deduplication, and curation are critical</p>
            </div>
        </div>

        <!-- Slide 16: Pre-training Process -->
        <div class="slide">
            <h1>Pre-training Process</h1>
            
            <h3>Infrastructure:</h3>
            <ul>
                <li><strong>Hardware:</strong> Thousands of GPUs/TPUs (A100s, H100s, TPU v4/v5)</li>
                <li><strong>Duration:</strong> Weeks to months of continuous training</li>
                <li><strong>Cost:</strong> Millions to tens of millions of dollars</li>
            </ul>
            
            <h3>Optimization:</h3>
            <ul>
                <li><strong>Optimizer:</strong> Adam, AdamW (weight decay variant)</li>
                <li><strong>Learning Rate:</strong> Warmup → Cosine decay schedule</li>
                <li><strong>Batch Size:</strong> Very large (millions of tokens per batch)</li>
                <li><strong>Gradient Accumulation:</strong> Simulate larger batches</li>
                <li><strong>Mixed Precision:</strong> FP16/BF16 for efficiency, FP32 for stability</li>
            </ul>
        </div>

        <!-- Slide 17: Distributed Training -->
        <div class="slide">
            <h1>Distributed Training Techniques</h1>
            
            <h3>Data Parallelism:</h3>
            <p>Replicate model across devices, split data batches</p>
            
            <h3>Model Parallelism:</h3>
            <p>Split model layers across devices (pipeline parallelism)</p>
            
            <h3>Tensor Parallelism:</h3>
            <p>Split individual layers/tensors across devices</p>
            
            <h3>ZeRO (Zero Redundancy Optimizer):</h3>
            <ul>
                <li><strong>ZeRO-1:</strong> Partition optimizer states</li>
                <li><strong>ZeRO-2:</strong> Partition gradients</li>
                <li><strong>ZeRO-3:</strong> Partition model parameters</li>
            </ul>
            
            <div class="highlight-box">
                <p><strong>3D Parallelism:</strong> Combine data + pipeline + tensor parallelism for maximum scale</p>
            </div>
        </div>

        <!-- Slide 18: Scaling Laws -->
        <div class="slide">
            <h1>Scaling Laws</h1>
            
            <h3>Key Findings (Kaplan et al., 2020):</h3>
            <p>Model performance follows predictable power laws with scale</p>
            
            <h3>Three Factors:</h3>
            <ul>
                <li><strong>Model Size (N):</strong> Number of parameters</li>
                <li><strong>Dataset Size (D):</strong> Number of training tokens</li>
                <li><strong>Compute (C):</strong> Total FLOPs for training</li>
            </ul>
            
            <div class="highlight-box">
                <p><strong>Chinchilla Scaling Laws:</strong> For compute-optimal training, model size and data should scale proportionally</p>
                <p>Previous models were undertrained! (GPT-3: 175B params, 300B tokens)</p>
                <p>Optimal: ~20 tokens per parameter (Chinchilla: 70B params, 1.4T tokens)</p>
            </div>
        </div>

        <!-- Slide 19: Emergent Abilities -->
        <div class="slide">
            <h1>Emergent Abilities</h1>
            
            <h3>Definition:</h3>
            <p>Capabilities that appear suddenly at certain scale thresholds, not present in smaller models</p>
            
            <h3>Examples:</h3>
            <ul>
                <li><strong>Few-shot learning:</strong> Learn from examples in prompt without fine-tuning</li>
                <li><strong>Chain-of-thought reasoning:</strong> Step-by-step problem solving</li>
                <li><strong>Multi-step reasoning:</strong> Complex logical deduction</li>
                <li><strong>Instruction following:</strong> Understanding and executing complex instructions</li>
                <li><strong>Code generation:</strong> Writing functional programs</li>
            </ul>
            
            <div class="highlight-box">
                <p><strong>Debate:</strong> Are these truly emergent or artifacts of evaluation metrics? Active research area</p>
            </div>
        </div>

        <!-- Slide 20: Fine-tuning -->
        <div class="slide">
            <h1>Fine-tuning</h1>
            
            <h3>Purpose:</h3>
            <p>Adapt pre-trained model to specific tasks or behaviors</p>
            
            <h3>Supervised Fine-Tuning (SFT):</h3>
            <ul>
                <li>Train on curated input-output pairs</li>
                <li>Teaches specific task formats and behaviors</li>
                <li>Much smaller dataset than pre-training (thousands to millions of examples)</li>
            </ul>
            
            <h3>Instruction Tuning:</h3>
            <ul>
                <li>Fine-tune on diverse instruction-following datasets</li>
                <li>Examples: FLAN, Alpaca, Dolly datasets</li>
                <li>Improves zero-shot task generalization</li>
            </ul>
        </div>

        <!-- Slide 21: RLHF Part 1 -->
        <div class="slide">
            <h1>RLHF: Reinforcement Learning from Human Feedback</h1>
            
            <h3>Motivation:</h3>
            <p>Next-token prediction alone doesn't ensure helpful, harmless, honest behavior</p>
            
            <h3>Three-Stage Process:</h3>
            <ol>
                <li><strong>Supervised Fine-Tuning (SFT):</strong> Train on high-quality demonstrations</li>
                <li><strong>Reward Model Training:</strong> Train model to predict human preferences</li>
                <li><strong>RL Optimization:</strong> Optimize policy using reward model</li>
            </ol>
            
            <div class="highlight-box">
                <p><strong>Used by:</strong> ChatGPT, Claude, GPT-4, Gemini — critical for conversational AI</p>
            </div>
        </div>

        <!-- Slide 22: RLHF Part 2 - Reward Model -->
        <div class="slide">
            <h1>RLHF: Reward Model</h1>
            
            <h3>Training the Reward Model:</h3>
            <ul>
                <li>Generate multiple responses to same prompt</li>
                <li>Humans rank responses by preference</li>
                <li>Train model to predict these rankings</li>
            </ul>
            
            <div class="equation">
                loss = -log(σ(r<sub>chosen</sub> - r<sub>rejected</sub>))
            </div>
            
            <h3>Output:</h3>
            <p>Scalar reward score for any (prompt, response) pair</p>
            
            <div class="highlight-box">
                <p><strong>Key Insight:</strong> Ranking is easier for humans than absolute scoring</p>
            </div>
        </div>

        <!-- Slide 23: RLHF Part 3 - PPO -->
        <div class="slide">
            <h1>RLHF: Policy Optimization</h1>
            
            <h3>PPO (Proximal Policy Optimization):</h3>
            <ul>
                <li>Generate responses using current policy (LLM)</li>
                <li>Score responses with reward model</li>
                <li>Update policy to maximize rewards</li>
                <li>Add KL penalty to prevent drift from original model</li>
            </ul>
            
            <div class="equation">
                objective = reward(prompt, response) - β·KL(π<sub>θ</sub> || π<sub>ref</sub>)
            </div>
            
            <h3>Challenges:</h3>
            <ul>
                <li>Reward hacking (exploiting reward model weaknesses)</li>
                <li>Instability during training</li>
                <li>Computational expense (4 models in memory)</li>
            </ul>
        </div>

        <!-- Slide 24: Alternative Alignment Methods -->
        <div class="slide">
            <h1>Alternative Alignment Methods</h1>
            
            <h3>DPO (Direct Preference Optimization):</h3>
            <ul>
                <li>Bypass reward model entirely</li>
                <li>Directly optimize policy on preference data</li>
                <li>Simpler, more stable than PPO</li>
                <li>Gaining popularity (used in LLaMA-2, Mistral)</li>
            </ul>
            
            <h3>Constitutional AI (Anthropic):</h3>
            <ul>
                <li>Model critiques and revises its own responses</li>
                <li>Guided by "constitution" of principles</li>
                <li>Reduces need for human feedback</li>
            </ul>
            
            <h3>RLAIF (RL from AI Feedback):</h3>
            <ul>
                <li>Use AI labelers instead of humans</li>
                <li>More scalable, consistent</li>
            </ul>
        </div>

        <!-- Slide 25: Model Architectures Comparison -->
        <div class="slide">
            <h1>Model Architecture Variants</h1>
            
            <table>
                <tr>
                    <th>Model Family</th>
                    <th>Architecture</th>
                    <th>Key Features</th>
                </tr>
                <tr>
                    <td>GPT Series</td>
                    <td>Decoder-only</td>
                    <td>Causal attention, autoregressive</td>
                </tr>
                <tr>
                    <td>BERT</td>
                    <td>Encoder-only</td>
                    <td>Bidirectional, masked language modeling</td>
                </tr>
                <tr>
                    <td>T5</td>
                    <td>Encoder-Decoder</td>
                    <td>Text-to-text framework</td>
                </tr>
                <tr>
                    <td>LLaMA</td>
                    <td>Decoder-only</td>
                    <td>RoPE, SwiGLU, RMSNorm</td>
                </tr>
                <tr>
                    <td>Claude</td>
                    <td>Decoder-only</td>
                    <td>Constitutional AI, long context</td>
                </tr>
            </table>
            
            <div class="highlight-box">
                <p><strong>Trend:</strong> Decoder-only models dominate modern LLM development</p>
            </div>
        </div>

        <!-- Slide 26: Inference -->
        <div class="slide">
            <h1>Inference & Generation</h1>
            
            <h3>Autoregressive Generation:</h3>
            <ol>
                <li>Input prompt → Forward pass → Logits</li>
                <li>Sample next token from probability distribution</li>
                <li>Append token to sequence</li>
                <li>Repeat until stop condition (max length, EOS token)</li>
            </ol>
            
            <h3>Sampling Strategies:</h3>
            <ul>
                <li><strong>Greedy:</strong> Always pick highest probability token (deterministic)</li>
                <li><strong>Temperature:</strong> Scale logits to control randomness (T→0: greedy, T→∞: uniform)</li>
                <li><strong>Top-k:</strong> Sample from top k tokens only</li>
                <li><strong>Top-p (nucleus):</strong> Sample from smallest set with cumulative probability ≥ p</li>
            </ul>
        </div>

        <!-- Slide 27: Optimization Techniques -->
        <div class="slide">
            <h1>Inference Optimization</h1>
            
            <h3>KV Cache:</h3>
            <p>Cache key-value pairs from previous tokens to avoid recomputation</p>
            
            <h3>Quantization:</h3>
            <ul>
                <li><strong>FP16/BF16:</strong> Half precision (minimal quality loss)</li>
                <li><strong>INT8:</strong> 8-bit integers (4x memory reduction)</li>
                <li><strong>INT4/GPTQ:</strong> 4-bit quantization for extreme compression</li>
            </ul>
            
            <h3>Batching:</h3>
            <ul>
                <li><strong>Static batching:</strong> Process multiple sequences together</li>
                <li><strong>Continuous batching:</strong> Add/remove sequences dynamically</li>
            </ul>
            
            <h3>Speculative Decoding:</h3>
            <p>Use small model to draft tokens, verify with large model (2-3x speedup)</p>
        </div>

        <!-- Slide 28: Context Length -->
        <div class="slide">
            <h1>Context Length & Long Context</h1>
            
            <h3>Challenge:</h3>
            <p>Self-attention is O(n²) in sequence length — expensive for long contexts</p>
            
            <h3>Solutions:</h3>
            <ul>
                <li><strong>Sparse Attention:</strong> Attend to subset of tokens (Longformer, BigBird)</li>
                <li><strong>Linear Attention:</strong> Approximate attention in O(n) (Linformer, Performer)</li>
                <li><strong>Sliding Window:</strong> Local attention windows (Mistral)</li>
                <li><strong>Flash Attention:</strong> Memory-efficient exact attention (GPU optimization)</li>
            </ul>
            
            <h3>Modern Capabilities:</h3>
            <ul>
                <li>GPT-4: 32k-128k tokens</li>
                <li>Claude: 100k-200k tokens</li>
                <li>Gemini 1.5: 1M+ tokens</li>
            </ul>
        </div>

        <!-- Slide 29: Parameter Efficient Fine-tuning -->
        <div class="slide">
            <h1>Parameter-Efficient Fine-Tuning (PEFT)</h1>
            
            <h3>Motivation:</h3>
            <p>Full fine-tuning is expensive — update only small subset of parameters</p>
            
            <h3>LoRA (Low-Rank Adaptation):</h3>
            <ul>
                <li>Freeze original weights</li>
                <li>Add trainable low-rank matrices: ΔW = AB (A: d×r, B: r×d, r≪d)</li>
                <li>Train only A and B (1-10% of parameters)</li>
                <li>Inference: W' = W + AB</li>
            </ul>
            
            <h3>Other Methods:</h3>
            <ul>
                <li><strong>Prefix Tuning:</strong> Add trainable prefix tokens</li>
                <li><strong>Adapter Layers:</strong> Insert small trainable modules</li>
                <li><strong>Prompt Tuning:</strong> Optimize soft prompts</li>
            </ul>
        </div>

        <!-- Slide 30: Multimodal Models -->
        <div class="slide">
            <h1>Multimodal Language Models</h1>
            
            <h3>Vision-Language Models:</h3>
            <ul>
                <li><strong>Architecture:</strong> Vision encoder (ViT/CLIP) + LLM decoder</li>
                <li><strong>Examples:</strong> GPT-4V, LLaVA, Flamingo, Gemini</li>
                <li><strong>Training:</strong> Image-text pairs, visual instruction tuning</li>
            </ul>
            
            <h3>Process:</h3>
            <ol>
                <li>Encode image into embeddings</li>
                <li>Project to LLM dimension</li>
                <li>Concatenate with text tokens</li>
                <li>Process through LLM</li>
            </ol>
            
            <div class="highlight-box">
                <p><strong>Trend:</strong> Unified models handling text, images, audio, video (e.g., GPT-4, Gemini)</p>
            </div>
        </div>

        <!-- Slide 31: Model Scale Comparison -->
        <div class="slide">
            <h1>Model Scale Comparison</h1>
            
            <table class="small-text">
                <tr>
                    <th>Model</th>
                    <th>Parameters</th>
                    <th>Training Tokens</th>
                    <th>Context Length</th>
                </tr>
                <tr>
                    <td>GPT-2</td>
                    <td>1.5B</td>
                    <td>~10B</td>
                    <td>1k</td>
                </tr>
                <tr>
                    <td>GPT-3</td>
                    <td>175B</td>
                    <td>300B</td>
                    <td>2k-4k</td>
                </tr>
                <tr>
                    <td>GPT-4</td>
                    <td>~1.7T (rumored)</td>
                    <td>~13T (estimated)</td>
                    <td>32k-128k</td>
                </tr>
                <tr>
                    <td>LLaMA-2</td>
                    <td>7B-70B</td>
                    <td>2T</td>
                    <td>4k</td>
                </tr>
                <tr>
                    <td>Claude 3</td>
                    <td>Unknown</td>
                    <td>Unknown</td>
                    <td>200k</td>
                </tr>
                <tr>
                    <td>Mistral 7B</td>
                    <td>7B</td>
                    <td>Unknown</td>
                    <td>8k-32k</td>
                </tr>
            </table>
        </div>

        <!-- Slide 32: Challenges -->
        <div class="slide">
            <h1>Current Challenges</h1>
            
            <h3>Technical:</h3>
            <ul>
                <li><strong>Hallucination:</strong> Generating plausible but false information</li>
                <li><strong>Reasoning Limitations:</strong> Struggle with complex multi-step reasoning</li>
                <li><strong>Factual Knowledge:</strong> Training data cutoff, outdated information</li>
                <li><strong>Context Length:</strong> Computational cost of long sequences</li>
                <li><strong>Inference Cost:</strong> Expensive to run large models at scale</li>
            </ul>
            
            <h3>Training:</h3>
            <ul>
                <li><strong>Data Quality:</strong> Web data is noisy, biased</li>
                <li><strong>Compute Requirements:</strong> Massive resources needed</li>
                <li><strong>Evaluation:</strong> Hard to measure true understanding</li>
            </ul>
        </div>

        <!-- Slide 33: Safety & Alignment -->
        <div class="slide">
            <h1>Safety & Alignment Challenges</h1>
            
            <h3>Key Issues:</h3>
            <ul>
                <li><strong>Bias:</strong> Models reflect biases in training data</li>
                <li><strong>Misuse:</strong> Generation of harmful content</li>
                <li><strong>Jailbreaking:</strong> Circumventing safety guardrails</li>
                <li><strong>Value Alignment:</strong> Ensuring models follow human values</li>
                <li><strong>Interpretability:</strong> Understanding model decisions</li>
            </ul>
            
            <h3>Mitigation Approaches:</h3>
            <ul>
                <li>RLHF and Constitutional AI</li>
                <li>Red teaming and adversarial testing</li>
                <li>Content filtering and moderation</li>
                <li>Mechanistic interpretability research</li>
            </ul>
        </div>

        <!-- Slide 34: Future Directions -->
        <div class="slide">
            <h1>Future Directions</h1>
            
            <h3>Architecture:</h3>
            <ul>
                <li><strong>Mixture of Experts (MoE):</strong> Conditional computation for efficiency</li>
                <li><strong>State Space Models:</strong> Alternative to Transformers (Mamba, RWKV)</li>
                <li><strong>Retrieval Augmentation:</strong> Integrate external knowledge dynamically</li>
            </ul>
            
            <h3>Training:</h3>
            <ul>
                <li><strong>Synthetic Data:</strong> Using AI to generate training data</li>
                <li><strong>Continual Learning:</strong> Update models with new information</li>
                <li><strong>Multi-task Learning:</strong> Single model for diverse tasks</li>
            </ul>
            
            <h3>Capabilities:</h3>
            <ul>
                <li><strong>Agentic Behavior:</strong> Planning, tool use, multi-step execution</li>
                <li><strong>Multimodal Understanding:</strong> Unified cross-modal reasoning</li>
            </ul>
        </div>

        <!-- Slide 35: Research Areas -->
        <div class="slide">
            <h1>Active Research Areas</h1>
            
            <div class="two-column">
                <div>
                    <h3>Efficiency:</h3>
                    <ul>
                        <li>Model compression</li>
                        <li>Efficient attention</li>
                        <li>Sparse models</li>
                        <li>Distillation</li>
                    </ul>
                    
                    <h3>Capabilities:</h3>
                    <ul>
                        <li>Reasoning enhancement</li>
                        <li>Factuality improvement</li>
                        <li>Long-term memory</li>
                        <li>Grounding</li>
                    </ul>
                </div>
                <div>
                    <h3>Safety:</h3>
                    <ul>
                        <li>Alignment research</li>
                        <li>Interpretability</li>
                        <li>Robustness</li>
                        <li>Bias mitigation</li>
                    </ul>
                    
                    <h3>Applications:</h3>
                    <ul>
                        <li>Code generation</li>
                        <li>Scientific discovery</li>
                        <li>Personalization</li>
                        <li>Multi-agent systems</li>
                    </ul>
                </div>
            </div>
        </div>

        <!-- Slide 36: Key Takeaways -->
        <div class="slide">
            <h1>Key Takeaways</h1>
            
            <div class="highlight-box">
                <h3>Architecture:</h3>
                <p>Transformers + Self-Attention enable parallel processing and long-range dependencies</p>
            </div>
            
            <div class="highlight-box">
                <h3>Training:</h3>
                <p>Self-supervised next-token prediction on massive datasets creates general capabilities</p>
            </div>
            
            <div class="highlight-box">
                <h3>Alignment:</h3>
                <p>RLHF and related techniques shape models to be helpful, harmless, and honest</p>
            </div>
            
            <div class="highlight-box">
                <h3>Scale:</h3>
                <p>Emergent abilities appear at scale — more parameters + data = better performance</p>
            </div>
        </div>

        <!-- Slide 37: Resources -->
        <div class="slide">
            <h1>Resources & Papers</h1>
            
            <h3>Foundational Papers:</h3>
            <ul class="small-text">
                <li>"Attention Is All You Need" (Vaswani et al., 2017)</li>
                <li>"Language Models are Few-Shot Learners" (GPT-3, Brown et al., 2020)</li>
                <li>"Training language models to follow instructions with human feedback" (InstructGPT, 2022)</li>
                <li>"LLaMA: Open and Efficient Foundation Language Models" (Touvron et al., 2023)</li>
                <li>"Scaling Laws for Neural Language Models" (Kaplan et al., 2020)</li>
            </ul>
            
            <h3>Learning Resources:</h3>
            <ul class="small-text">
                <li>Stanford CS224N: Natural Language Processing with Deep Learning</li>
                <li>Hugging Face NLP Course</li>
                <li>The Illustrated Transformer (Jay Alammar)</li>
                <li>Andrej Karpathy's Neural Networks: Zero to Hero</li>
            </ul>
        </div>

        <!-- Slide 38: Thank You -->
        <div class="slide title-slide">
            <h1>Thank You!</h1>
            <p style="margin-top: 50px; font-size: 1.5em;">Questions?</p>
        </div>
    </div>

    <div class="slide-number"></div>
    
    <div class="controls">
        <button id="prev">← Previous</button>
        <button id="next">Next →</button>
    </div>

    <script>
        let currentSlide = 0;
        const slides = document.querySelectorAll('.slide');
        const slideNumber = document.querySelector('.slide-number');
        
        function showSlide(n) {
            slides[currentSlide].classList.remove('active');
            currentSlide = (n + slides.length) % slides.length;
            slides[currentSlide].classList.add('active');
            slideNumber.textContent = `${currentSlide + 1} / ${slides.length}`;
        }
        
        document.getElementById('next').addEventListener('click', () => {
            showSlide(currentSlide + 1);
        });
        
        document.getElementById('prev').addEventListener('click', () => {
            showSlide(currentSlide - 1);
        });
        
        document.addEventListener('keydown', (e) => {
            if (e.key === 'ArrowRight' || e.key === ' ') {
                showSlide(currentSlide + 1);
            } else if (e.key === 'ArrowLeft') {
                showSlide(currentSlide - 1);
            }
        });
        
        // Initialize
        showSlide(0);
    </script>
</body>
</html>